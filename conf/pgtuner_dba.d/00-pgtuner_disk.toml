## Note that in thi list, we choose the value based on the minimum of read/write IOPS/throughput, and doing
## some averaging across value (and smally reduce the number); so the value here may not be reflected on your disk
## storage. But thinking about disk degradation after time and other factor. Thus to make use of this, it is best to
## choose based on the nearest value of minimum of read/write IOPS/throughput and round down (not up).

## These values are gained from TechPowerUp's SSD IOPS Database.
## Please note that the naming model with suffix _v1, _v2, _v3, etc. is used to differentiate the IOPS value based on
## the read/write ratio, rather than the comparison of the disk itself. Also, we don't take into the consideration of
## DRAM-availabiltiy on the SSD as we are focusing on the disk read/write after (p)SLC cache is full and not full.
## For NVME drive, when pseudo-SLC cache is full, unless you are using too small drive such as 128 GB or 256 GB, or flaw
## SSD with corrupted pSLC, or the drive is nearly full (80% full) the drive could write as around 60-80% of the
## throughput provided by the manufacturer

## For random IOPS or throughput metric, we never achieve their advertised value due to the noise, external conditions,
## but importantly the OS or driver operations, and the page-size of the I/O request. For example, the WD SN770 1 TiB
## NVME PCIe 4.0x4 has 740k IOPs read and 800k IOPs write, but measurement shows on 4K request as data disk that it
## could achieve maximum 180K IOPs and 125K - 155K for mixed read/write (50% - 99%) at 80% full capacity. Please note
## that SSD performance is degraded overtime when the drive capacity is running out due to usage, so it is best
## not to use the advertised value


[pgtuner_dba.disk.throughput]
# Throughput of the disk in MB/s
hddv1 = 100      # Some old HDDs or SD cards
hddv2 = 250      # Some Western Digital HDDs, or modern HDDs with small SSD/DRAM cache (220 - 290 MiB/s)

ssdv1 = 350
ssdv2 = 500
ssdv3 = 525
ssdv4 = 550
ssdv5 = 550
ssdv6 = 550
ssdv7 = 550

# We don't make custom value of NVME SSD such as PCIe 3.0/4.0 x8 or 3.0/4.0 x16, as it's extremely rare to nothing.
# Also, if you have that drive, it is best that you should think as RAID 0/10 and scale the number instead of having
# custom value here. For example, on the same category end-range NVME SSD, the nvmepciev3x8v2 would have twice the
# throughput of nvmepciev3x4v2, and nvmepciev3x16v2 would have twice the throughput of nvmepciev3x8v2.
nvmepciev3x4v1 = 2000
nvmepciev3x4v2 = 2500
nvmepciev3x4v3 = 3000
nvmepciev3x4v4 = 3500
nvmepciev3x4v5 = 4000
nvmepciev3x4v6 = 4500

nvmepciev4x4v1 = 4500
nvmepciev4x4v2 = 5000
nvmepciev4x4v3 = 5500
nvmepciev4x4v4 = 6000
nvmepciev4x4v5 = 6500
nvmepciev4x4v6 = 7000

nvmepciev5x4v1 = 7000
nvmepciev5x4v2 = 8500
nvmepciev5x4v3 = 9500
nvmepciev5x4v4 = 11000
nvmepciev5x4v5 = 12500
nvmepciev5x4v6 = 14000

[pgtuner_dba.disk.random_iops]
# Random IOPS of the disk. Note that IOPS is not a linear scale, and it is best to choose the nearest value of the
# minimum of read/write IOPS and round down (not up). Some Western Digital HDDs, or modern HDDs with small SSD/DRAM
# cache brings more IOPS https://www.youtube.com/watch?v=lc8CcHnf5HA
hddv1 = 70
hddv2 = 250

ssdv1 = 20000
ssdv2 = 30000
ssdv3 = 40000
ssdv4 = 50000
ssdv5 = 65000
ssdv6 = 80000
ssdv7 = 90000

nvmepciev3x4v1 = 150000
nvmepciev3x4v2 = 200000
nvmepciev3x4v3 = 250000
nvmepciev3x4v4 = 300000
nvmepciev3x4v5 = 350000
nvmepciev3x4v6 = 400000

nvmepciev4x4v1 = 300000
nvmepciev4x4v2 = 375000
nvmepciev4x4v3 = 450000
nvmepciev4x4v4 = 525000
nvmepciev4x4v5 = 600000
nvmepciev4x4v6 = 700000

nvmepciev5x4v1 = 700000
nvmepciev5x4v2 = 850000
nvmepciev5x4v3 = 950000
nvmepciev5x4v4 = 1100000
nvmepciev5x4v5 = 1250000
nvmepciev5x4v6 = 1400000
